{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/elliottabe/RF_Workshop/blob/main/Workshop_notebook.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Installing repo and dependencies if using colab. Skip to imports if running locally\n",
    "!pip install -U matplotlib &> /dev/null\n",
    "!git clone https://github.com/elliottabe/RF_workshop.git &> /dev/null\n",
    "!pip install -r ./RF_workshop/requirements.txt &> /dev/null\n",
    "# !pip install git+https://github.com/elliottabe/RF_workshop.git &> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "file_id = '1AUYAmfQp3Hh25uf_mohaT3N3qLXKlMeo' # File id to example data\n",
    "output_file = 'data.h5'\n",
    "\n",
    "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "import io_dict_to_hdf5 as ioh5\n",
    "\n",
    "##### Plotting settings ######\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams.update({'font.size':         10,\n",
    "                     'axes.linewidth':    2,\n",
    "                     'xtick.major.size':  3,\n",
    "                     'xtick.major.width': 2,\n",
    "                     'ytick.major.size':  3,\n",
    "                     'ytick.major.width': 2,\n",
    "                     'axes.spines.right': False,\n",
    "                     'axes.spines.top':   False,\n",
    "                     'pdf.fonttype':      42,\n",
    "                     'xtick.labelsize':   10,\n",
    "                     'ytick.labelsize':   10,\n",
    "                     'figure.facecolor': 'white'\n",
    "\n",
    "                    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ioh5.load('./data.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nsp = data['model_nsp']\n",
    "model_vid_sm = data['model_vid_sm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nsp.shape, model_vid_sm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NKfold = 1\n",
    "test_train_size = 0.8\n",
    "frac = 0.1\n",
    "gss = GroupShuffleSplit(n_splits=NKfold, train_size=test_train_size, random_state=42)\n",
    "nT = model_nsp.shape[0]\n",
    "groups = np.hstack([i*np.ones(int((frac*i)*nT) - int((frac*(i-1))*nT)) for i in range(1,int(1/frac)+1)])\n",
    "\n",
    "train_idx_list=[]\n",
    "test_idx_list = []\n",
    "for train_idx, test_idx in gss.split(np.arange(nT), groups=groups):\n",
    "    train_idx_list.append(train_idx)\n",
    "    test_idx_list.append(test_idx)\n",
    "    \n",
    "cropn = 0\n",
    "train_idx = train_idx_list[0]\n",
    "test_idx = test_idx_list[0]\n",
    "if cropn>0:\n",
    "    xtrain = model_vid_sm[train_idx][:,cropn:-cropn,cropn:-cropn]\n",
    "    xtest = model_vid_sm[test_idx][:,cropn:-cropn,cropn:-cropn]\n",
    "else: \n",
    "    xtrain = model_vid_sm[train_idx]\n",
    "    xtest = model_vid_sm[test_idx]\n",
    "im_size = xtrain.shape[1:]\n",
    "xtrain = xtrain.reshape(len(train_idx),-1)\n",
    "xtest = xtest.reshape(len(test_idx),-1)\n",
    "ytrain = model_nsp[train_idx]\n",
    "ytest = model_nsp[test_idx]\n",
    "\n",
    "xtrain.shape, ytrain.shape, xtest.shape, ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr, xte, ytr, yte = torch.from_numpy(xtrain).float().to(device), torch.from_numpy(xtest).float().to(device), torch.from_numpy(ytrain).float().to(device), torch.from_numpy(ytest).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = xtr.shape[1]\n",
    "output_size = ytr.shape[1]\n",
    "Num_units = model_nsp.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Pytorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(input_size,output_size),\n",
    "                      nn.ReLU()).to(device)\n",
    "optimizer = torch.optim.RAdam(model.parameters(), lr=.001, weight_decay=.1)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=.001, weight_decay=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nepochs = 2000\n",
    "l2_lambda_list = [.05,.1,1]\n",
    "l1_alpha = 0.0001\n",
    "min_loss = np.inf\n",
    "with tqdm(initial=0,total=len(l2_lambda_list), dynamic_ncols=False, miniters=1) as tq:\n",
    "    for l2_lambda in l2_lambda_list:\n",
    "        model = nn.Sequential(nn.Linear(input_size,output_size),\n",
    "                            nn.ReLU()).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=.001, weight_decay=l2_lambda)\n",
    "        \n",
    "        for epoch in tqdm(range(Nepochs),leave=False):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            yhat = model(xtr)\n",
    "            train_loss = nn.MSELoss()(yhat, ytr) + l1_alpha*torch.norm(model[0].weight,p=1)\n",
    "            train_loss.backward(torch.ones_like(train_loss))\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "        \n",
    "        yhat = model(xte)\n",
    "        val_loss = nn.MSELoss()(yhat, yte)  + l1_alpha*torch.norm(model[0].weight,p=1)\n",
    "\n",
    "        if val_loss < min_loss:\n",
    "            l2_lambda_min = l2_lambda\n",
    "            torch.save(model.state_dict(),'./RF_l2_min.pt')\n",
    "            min_loss = val_loss\n",
    "            \n",
    "        tq.set_postfix(val_loss='{:05.3f}'.format(val_loss),train_loss='{:05.3f}'.format(train_loss),min_loss='{:05.3f}'.format(min_loss))\n",
    "        tq.update()\n",
    "load_model = torch.load('./RF_l2_min.pt')\n",
    "model.load_state_dict(load_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = model[0].weight.detach().cpu().numpy().reshape(Num_units,im_size[0],im_size[1])\n",
    "RF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(13,10,figsize=(20,20))\n",
    "for n, ax in enumerate(range(RF.shape[0])):\n",
    "    ax = axs.flatten()[n]\n",
    "    cmax = np.max(np.abs(RF[n]))\n",
    "    ax.imshow(RF[n],cmap='RdBu_r',vmin=-cmax,vmax=cmax)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'{n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_vis = data['RF_vis']\n",
    "\n",
    "RF_vis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(13,10,figsize=(20,20))\n",
    "for n, ax in enumerate(range(RF_vis.shape[0])):\n",
    "    ax = axs.flatten()[n]\n",
    "    cmax = np.max(np.abs(RF_vis[n]))\n",
    "    ax.imshow(RF_vis[n,2],cmap='RdBu_r',vmin=-cmax,vmax=cmax)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'{n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional STA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr[torch.isnan(xtr)] = 0\n",
    "\n",
    "# fig = plt.figure(figsize=(20, np.ceil(n_units/2)))\n",
    "sta_all = np.zeros((Num_units,im_size[0],im_size[1]))\n",
    "for c in range(Num_units):\n",
    "\n",
    "    sp = ytr[:,c].clone().unsqueeze(1)\n",
    "    # sp = np.roll(sp, -lag)\n",
    "    sta = xtr.T @ sp\n",
    "    sta = torch.reshape(sta, im_size)\n",
    "    nsp = torch.sum(sp)\n",
    "\n",
    "    # plt.subplot(int(np.ceil(n_units/10)), 10, c+1)\n",
    "\n",
    "    if nsp > 0:\n",
    "\n",
    "        sta = sta/nsp\n",
    "        # flip matrix so that physical top is at the top (worldcam comes in upsidedown)\n",
    "        # sta = np.fliplr(np.flipud(sta))\n",
    "    sta_all[c] = sta.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(13,10,figsize=(20,20))\n",
    "for n, ax in enumerate(range(sta_all.shape[0])):\n",
    "    ax = axs.flatten()[n]\n",
    "    cmax = np.max(np.abs(sta_all[n]))\n",
    "    ax.imshow(sta_all[n],cmap='RdBu_r',vmin=-cmax,vmax=cmax)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'{n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchGLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
