{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elliottabe/pytorchGLM/blob/main/Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aZYSqn4bnBmY"
      },
      "outputs": [],
      "source": [
        "#@title Installing repo and dependencies if using colab. Skip to imports if running locally\n",
        "!pip install -U matplotlib &> /dev/null\n",
        "!git clone https://github.com/elliottabe/pytorchGLM.git &> /dev/null\n",
        "!pip install -r ./pytorchGLM/requirements.txt &> /dev/null\n",
        "!pip install git+https://github.com/elliottabe/pytorchGLM.git &> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_My6Ew-kvJI8"
      },
      "outputs": [],
      "source": [
        "#@title Download Niell lab Datasets\n",
        "\n",
        "import zipfile\n",
        "from IPython.display import clear_output\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1X1dpHDy0zblsVmW-8lC6Vji8g2x0s0bY' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1X1dpHDy0zblsVmW-8lC6Vji8g2x0s0bY\" -O 070921.zip && rm -rf /tmp/cookies.txt\n",
        "\n",
        "with zipfile.ZipFile('/home/eabe/Research/MyRepos/RF_Workshop/070921.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('Testing')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeRIR-2yAdm0"
      },
      "source": [
        "Restart Runtime before continuing!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPRY9jaSm56W"
      },
      "source": [
        "# Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VVU37i6dm56Y"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "torch.backends.cudnn.benchmark = True\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "##### Plotting settings ######\n",
        "import matplotlib as mpl\n",
        "\n",
        "mpl.rcParams.update({'font.size':         10,\n",
        "                     'axes.linewidth':    2,\n",
        "                     'xtick.major.size':  3,\n",
        "                     'xtick.major.width': 2,\n",
        "                     'ytick.major.size':  3,\n",
        "                     'ytick.major.width': 2,\n",
        "                     'axes.spines.right': False,\n",
        "                     'axes.spines.top':   False,\n",
        "                     'pdf.fonttype':      42,\n",
        "                     'xtick.labelsize':   10,\n",
        "                     'ytick.labelsize':   10,\n",
        "                     'figure.facecolor': 'white'\n",
        "\n",
        "                    })\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Audio Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.io import loadmat\n",
        "\n",
        "data_path = Path('/mnt/c/Users/corve/Downloads/NeuralData/')\n",
        "data_files = list(data_path.glob('*.mat'))\n",
        "data_files \n",
        "spTimes = loadmat(data_files[2])\n",
        "freq = loadmat(data_files[0])\n",
        "TLvl = loadmat(data_files[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4084"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spTimes['spTimes'].shape[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[array([[array([[0.01268096, 0.01522048, 0.1164736 , 0.11856256, 0.17275264,\n",
              "                        0.17611136, 0.1770944 , 0.17869184, 0.1816    , 0.18307456]]),\n",
              "                array([[-0.08431232, -0.00874112, -0.00300672,  0.01153408,  0.050528  ,\n",
              "                         0.06355328,  0.0716224 ,  0.08038784,  0.09111936,  0.11590016,\n",
              "                         0.17377664,  0.17779072,  0.17848704,  0.17979776,  0.18069888,\n",
              "                         0.18577792]])                                                  ,\n",
              "                array([[-0.08976   ,  0.01812864,  0.02177408,  0.02582912,  0.02791808,\n",
              "                         0.05458304,  0.06973824,  0.1164736 ,  0.11839872,  0.1852864 ,\n",
              "                         0.19523968]])                                                  ,\n",
              "                array([[-0.07374464, -0.02705024, -0.01013376,  0.02288   ,  0.02754944,\n",
              "                         0.06678912,  0.12274048,  0.17447296,  0.1779136 ,  0.18196864,\n",
              "                         0.18418048,  0.18577792,  0.1883584 ,  0.19085696,  0.19274112]]),\n",
              "                array([[-0.00239232,  0.08599936,  0.1164736 ,  0.178528  ,  0.18254208,\n",
              "                         0.18418048,  0.188768  ,  0.19302784]])                        ,\n",
              "                array([], shape=(0, 0), dtype=uint8)]], dtype=object)                      ,\n",
              "        array([[array([[-0.07825024, -0.07403136, -0.07247488, -0.06198912, -0.05723776,\n",
              "                        -0.0168512 ,  0.026976  ,  0.09767296,  0.09910656,  0.10238336]]),\n",
              "                array([[-0.07960192, -0.07784064, -0.0760384 , -0.07411328, -0.06674048,\n",
              "                         0.04434304,  0.04700544,  0.06809984]])                        ,\n",
              "                array([[-0.07612032, -0.0526912 , -0.02967168, -0.0256576 , -0.022176  ,\n",
              "                        -0.0168512 , -0.01422976,  0.09955712]])                        ,\n",
              "                array([[-0.0451136 , -0.03884672,  0.00723328,  0.06646144,  0.07473536,\n",
              "                         0.10996096]])                                                  ,\n",
              "                array([[-0.00284288,  0.13052288]]),\n",
              "                array([[-0.04834944,  0.0738752 ,  0.13056384]])]], dtype=object)          ,\n",
              "        array([[array([[0.02189696, 0.04880768, 0.11745664, 0.12745088, 0.13003136,\n",
              "                        0.14154112, 0.14702976, 0.15305088]])                      ,\n",
              "                array([[-0.08795776, -0.08558208, -0.05367424, -0.02672256, -0.00345728,\n",
              "                         0.0292288 ,  0.031072  ,  0.08849792,  0.10127744]])           ,\n",
              "                array([[-0.09270912, -0.08705664, -0.08529536, -0.05363328, -0.04257408,\n",
              "                        -0.03782272, -0.03556992, -0.02672256,  0.01886592,  0.03111296,\n",
              "                         0.04266368,  0.100704  ,  0.10844544,  0.11737472,  0.12695936,\n",
              "                         0.13007232,  0.13498752,  0.14305664,  0.15042944]])           ,\n",
              "                array([[-0.04073088, -0.03962496, -0.03782272, -0.03294848, -0.0217664 ,\n",
              "                        -0.00726656,  0.0286144 ,  0.03856768,  0.09468288,  0.13576576,\n",
              "                         0.16755072,  0.18123136,  0.19331456]])                        ,\n",
              "                array([[-0.08529536, -0.06932096, -0.0606784 , -0.05731968, -0.04748928,\n",
              "                        -0.00345728,  0.0122304 ,  0.04774272,  0.05659008,  0.06859136,\n",
              "                         0.08853888,  0.12257664,  0.13998464,  0.14305664,  0.17664384,\n",
              "                         0.19806592]])                                                  ,\n",
              "                array([], shape=(0, 0), dtype=uint8)]], dtype=object)                    ,\n",
              "        ..., array([], shape=(1, 0), dtype=float64),\n",
              "        array([], shape=(1, 0), dtype=float64),\n",
              "        array([], shape=(1, 0), dtype=float64)]], dtype=object)"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spTimes['spTimes']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "spkTimes = [[spTimes['spTimes'][0,trial][0,channel].squeeze() for trial in range(3888)] for channel in range(6)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([array([0.01268096, 0.01522048, 0.1164736 , 0.11856256, 0.17275264,\n",
              "              0.17611136, 0.1770944 , 0.17869184, 0.1816    , 0.18307456]),\n",
              "       array([-0.07825024, -0.07403136, -0.07247488, -0.06198912, -0.05723776,\n",
              "              -0.0168512 ,  0.026976  ,  0.09767296,  0.09910656,  0.10238336]),\n",
              "       array([0.02189696, 0.04880768, 0.11745664, 0.12745088, 0.13003136,\n",
              "              0.14154112, 0.14702976, 0.15305088])                       ,\n",
              "       ..., array([], shape=(0, 0), dtype=uint8),\n",
              "       array([0.08587648, 0.08702336, 0.18274688, 0.18901376]),\n",
              "       array([], shape=(0, 0), dtype=uint8)], dtype=object)"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.stack(spkTimes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dict = {'spTimes': spkTimes, 'freq': freq['freq'], 'TLvl': TLvl['TLvl']}\n",
        "import io_dict_to_hdf5 as ioh5\n",
        "ioh5.save('./NeuralData.h5',data_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "neuralData = ioh5.load('NeuralData.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.01268096, 0.01522048, 0.1164736 , 0.11856256, 0.17275264,\n",
              "       0.17611136, 0.1770944 , 0.17869184, 0.1816    , 0.18307456])"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "neuralData['spTimes']['0']['0']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4ciojQam56Z"
      },
      "source": [
        "# Format Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viUJ0-_sm56a"
      },
      "source": [
        "## Loading  Niell lab Formatted Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSTLL7Yem56a"
      },
      "outputs": [],
      "source": [
        "# Input arguments\n",
        "args = pglm.arg_parser(jupyter=True)\n",
        "\n",
        "##### Modify default argments if needed #####\n",
        "dates_all = ['070921/J553RT' ,'101521/J559NC','102821/J570LT','110421/J569LT'] #,'122021/J581RT','020422/J577RT'] # '102621/J558NC' '062921/G6HCK1ALTRN',\n",
        "args['date_ani']        = dates_all[0]\n",
        "args['base_dir']        = './Testing'\n",
        "args['fig_dir']         = './FigTesting'\n",
        "args['data_dir']        = './'\n",
        "args['free_move']       = True\n",
        "args['train_shifter']   = False\n",
        "args['Nepochs']         = 1000\n",
        "\n",
        "ModelID = 1\n",
        "params, file_dict, exp = pglm.load_params(args,ModelID,file_dict={},exp_dir_name=None,nKfold=0,debug=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lyyGCFDm56b"
      },
      "outputs": [],
      "source": [
        "##### Load Niell Lab Dataset #####\n",
        "data = pglm.load_aligned_data(file_dict, params, reprocess=False)\n",
        "params = pglm.get_modeltype(params)\n",
        "datasets, network_config,initial_params = pglm.load_datasets(file_dict,params,single_trial=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Z_kw1eYm56c"
      },
      "outputs": [],
      "source": [
        "##### Explore data shape #####\n",
        "##### xtr: (time,features), xtr_pos:(time,features), ytr:(time,neurons)\n",
        "x,xpos,y = datasets['xtr'][:10],datasets['xtr_pos'][:10],datasets['ytr'][:10]\n",
        "print(x.shape,xpos.shape,y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uu9K4RV4NBD"
      },
      "source": [
        "## Single Trial Training on Niell Lab data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbUHetx32-Qa"
      },
      "outputs": [],
      "source": [
        "##### Train GLM #####\n",
        "tloss_trace,vloss_trace,model,optimizer = train_network(network_config,**datasets, params=params,filename=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWIlsFiE3iFX"
      },
      "outputs": [],
      "source": [
        "##### Plotting RFs #####\n",
        "RFs = model.Cell_NN[0].weight.reshape((params['Ncells'],params['nt_glm_lag'])+params['nks']).cpu().detach().numpy()\n",
        "cells = [22,34,42,101]\n",
        "lag_list = [0,1,2,3,4]\n",
        "lag_ls = [-100,-50,0,50,100]\n",
        "fig, axs = plt.subplots(4,5,figsize=(10,5))\n",
        "for n, cell in enumerate(cells):\n",
        "    crange = np.max(np.abs(RFs[cell]))\n",
        "    for m,lag in enumerate(lag_list):\n",
        "        ax = axs[n,m]\n",
        "        im = ax.imshow(RFs[cell,lag],'RdBu_r',vmin=-crange,vmax=crange)\n",
        "        axs[0,m].set_title('{}ms'.format(lag_ls[m]),fontsize=10)\n",
        "\n",
        "\n",
        "cbar2 = fig.colorbar(im, ax=axs,shrink=.8)\n",
        "cbar2.set_ticks([-crange, crange])\n",
        "cbar2.set_ticklabels(['dark', 'light'])\n",
        "cbar2.ax.tick_params(labelsize=10, rotation=90,width=0,length=0)\n",
        "cbar2.outline.set_linewidth(1)\n",
        "\n",
        "for ax in axs.flat:\n",
        "    for axis in ['top','bottom','left','right']:\n",
        "        ax.spines[axis].set_linewidth(0.5)\n",
        "        ax.spines[axis].set_visible(True)\n",
        "    ax.set_xticklabels([])\n",
        "    ax.set_yticklabels([])\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eGfOxpZ5lpW"
      },
      "source": [
        "## Formatting Raw Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w36ktj7Tm56c"
      },
      "source": [
        "load_aligned_data calls 2 functions when preprocessing the raw data: \n",
        "- format_raw_data: formats the raw data based on file_dict and params\n",
        "- interp_raw_data: interpolates the formamted data from format_raw_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wM2y5eSHm56d"
      },
      "outputs": [],
      "source": [
        "##### Preprocess raw data from Niell Lab #####\n",
        "raw_data, goodcells = pglm.format_raw_data(file_dict,params)\n",
        "model_data = pglm.interp_raw_data(raw_data,raw_data['vid']['vidTS'],model_dt=params['model_dt'],goodcells=goodcells)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xogFmWNrm56d"
      },
      "source": [
        "## Custom Dataset Formatting Base Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhYs9qS8m56e"
      },
      "source": [
        "This section is dedicated to formatting any custom datasets. The key components are: \n",
        "- Formatting data\n",
        "    - Train/Test Splits\n",
        "    - Inputs (time,in_features)\n",
        "    - Optional Inputs (time,pos_features)\n",
        "    - Outputs (time,Ncells)\n",
        "- network_config\n",
        "    - in_features: input dims\n",
        "    - Ncells: output dims\n",
        "    - initW: How to initialize weights, 'zero' or 'normal' \n",
        "    - optimizer: optmimizer to use: 'adam' or 'sgd'\n",
        "    - lr_w: learning rate for weights\n",
        "    - lr_b: learning rate for bias\n",
        "    - lr_m: learning rate for additional inputs\n",
        "    - single_trial: flag for single trial or hyperparam seach\n",
        "    - L1_alpha: L1 regularization parameter. Single value or hyperparam search\n",
        "    - L1_alpham: L1 regularization parameter. Single value or hyperparam search\n",
        "    - L2_lambda: L2 regularization parameter. Single value or hyperparam search\n",
        "    - L2_lambda_m: L2 regularization parameter. Single value or hyperparam search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwDs5qQjm56e"
      },
      "outputs": [],
      "source": [
        "args = pglm.arg_parser(jupyter=True)\n",
        "args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "D-YI107Jm56e"
      },
      "outputs": [],
      "source": [
        "#@title Custom load functions\n",
        "\n",
        "def load_BaseModel_params(args,exp_dir_name='Testing',ModelID=0,nKfold=0,debug=False):\n",
        "    \"\"\" Load parameter dictionary for custom BaseModel network. Minimal implementation \n",
        "        adabpting to custom datasets\n",
        "\n",
        "    Args:\n",
        "        args (dict): Argument dictionary \n",
        "        exp_dir_name (str): name of experiment. \n",
        "        ModelID (int, optional): Model Identification number. Defaults to 0.\n",
        "        exp_dir_name (str, optional): Optional experiment directory name if using own data. Defaults to None.\n",
        "        nKfold (int, optional): Kfold number for versioning. Defaults to 0.\n",
        "        debug (bool, optional): debug=True does not create experiment directories. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        params (dict): dictionary of parameters\n",
        "        exp (obj): Test_tube object for organizing files and tensorboard\n",
        "    \"\"\"\n",
        "    import yaml\n",
        "    from pathlib import Path\n",
        "    from test_tube import Experiment\n",
        "    \n",
        "    ##### Create directories and paths #####\n",
        "    date_ani2 = '_'.join(args['date_ani'].split('/'))\n",
        "    data_dir = Path(args['data_dir']).expanduser() / args['date_ani'] / args['stim_cond'] \n",
        "    base_dir = Path(args['base_dir']).expanduser()\n",
        "    save_dir = (base_dir / args['date_ani'] / args['stim_cond'])\n",
        "    save_dir.mkdir(parents=True, exist_ok=True)\n",
        "    base_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    ##### Set up test_tube versioning #####\n",
        "    exp = Experiment(name='ModelID{}'.format(ModelID),\n",
        "                        save_dir=save_dir / exp_dir_name, \n",
        "                        debug=debug,\n",
        "                        version=nKfold)\n",
        "\n",
        "    save_model = exp.save_dir / exp.name / 'version_{}'.format(nKfold)\n",
        "\n",
        "    params = {\n",
        "        ##### Data Parameters #####\n",
        "        'data_dir':                 data_dir,\n",
        "        'base_dir':                 base_dir,\n",
        "        'exp_name_base':            base_dir.name,\n",
        "        'stim_cond':                args['stim_cond'],\n",
        "        'save_dir':                 save_dir,\n",
        "        'exp_name':                 exp.save_dir.name,\n",
        "        'save_model':               save_model,\n",
        "        'date_ani2':                date_ani2,\n",
        "        'model_dt':                 args['model_dt'],\n",
        "        ##### Model Parameters #####\n",
        "        'ModelID':                  ModelID,\n",
        "        'lag_list':                 [0], # List of which timesteps to include in model fit\n",
        "        'Nepochs':                  args['Nepochs'],\n",
        "        'Kfold':                    args['Kfold'],\n",
        "        'NoL1':                     args['NoL1'],\n",
        "        'NoL2':                     args['NoL2'],\n",
        "        'initW':                    'zero',\n",
        "        'train_shifter':            False,\n",
        "        'model_type':               'pytorchGLM_custom', # For naming files\n",
        "    }\n",
        "\n",
        "    params['nt_glm_lag']=len(params['lag_list']) # number of timesteps for model fits\n",
        "    params['data_name'] = '_'.join([params['date_ani2'],params['stim_cond']])\n",
        "    \n",
        "    ##### Saves yaml of parameters #####\n",
        "    if debug==False:\n",
        "        params2=params.copy()\n",
        "        for key in params2.keys():\n",
        "            if isinstance(params2[key], Path):\n",
        "                params2[key]=params2[key].as_posix()\n",
        "\n",
        "        pfile_path = save_model / 'model_params.yaml'\n",
        "        with open(pfile_path, 'w') as file:\n",
        "            doc = yaml.dump(params2, file, sort_keys=True)\n",
        "\n",
        "    return params, exp\n",
        "\n",
        "\n",
        "def initialize_GP_inputs(Npats,length_scale,batch_size,Nx_low,Nx,Ny_star,Nr,seed=42,multi_input=False,pytorch=True):\n",
        "    from sklearn.gaussian_process.kernels import RBF\n",
        "\n",
        "    ##### Set random seed #####\n",
        "    np.random.seed(seed+1)\n",
        "    torch.manual_seed(seed+1)\n",
        "    ##### Initialize RBF kernels #####\n",
        "    rbf = RBF(length_scale=length_scale)\n",
        "    genX = np.arange(Npats)[:,np.newaxis]\n",
        "    genY = np.arange(Npats)[:,np.newaxis]\n",
        "    Kx = rbf(genX,genX)\n",
        "    Ky = rbf(genY,genY)\n",
        "    if multi_input:\n",
        "        ##### Initialize inputs #####\n",
        "        x_low0 = torch.transpose(torch.from_numpy(np.random.multivariate_normal(np.zeros(Npats), Kx,size=(batch_size,Nx_low))),2,1).float()\n",
        "        x_low1 = torch.transpose(torch.from_numpy(np.random.multivariate_normal(np.zeros(Npats), Kx,size=(batch_size,Nx_low))),2,1).float()\n",
        "        x_expand = torch.randn(size=(batch_size,Nx_low,Nx)).float()\n",
        "        x0 = torch.bmm(x_low0,x_expand)\n",
        "        x1 = torch.bmm(x_low1,x_expand)\n",
        "        x_all = torch.stack((x0,x1),dim=1).float()\n",
        "        ##### Initialize target patterns #####\n",
        "        y_all = torch.transpose(torch.from_numpy(np.random.multivariate_normal(np.zeros(Npats), Ky,size=(1,Ny_star,Nr))),3,2)\n",
        "        y_all = ((y_all/torch.max(torch.max(torch.abs(y_all),dim=1,keepdim=True)[0],dim=2,keepdim=True)[0]).repeat(batch_size,1,1,1))\n",
        "    else:\n",
        "        ##### Initialize inputs #####\n",
        "        x_low0 = torch.transpose(torch.from_numpy(np.random.multivariate_normal(np.zeros(Npats), Kx,size=(batch_size,Nx_low))),2,1).float()\n",
        "        x_expand = torch.randn(size=(batch_size,Nx_low,Nx)).float()\n",
        "        x_all = torch.bmm(x_low0,x_expand)#.numpy()\n",
        "        # x_all = torch.from_numpy((x_all - np.nanmean(x_all,axis=0))/np.nanstd(x_all,axis=0)).float()\n",
        "        ##### Initialize target patterns #####\n",
        "        y_all = torch.from_numpy(np.random.multivariate_normal(np.zeros(Npats), Ky,size=(1,Nr)))\n",
        "        y_all = torch.transpose((y_all/torch.max(torch.max(torch.abs(y_all),dim=1,keepdim=True)[0],dim=2,keepdim=True)[0]).repeat(batch_size,1,1),-1,-2)\n",
        "\n",
        "    if pytorch:\n",
        "        x_all = x_all.float()\n",
        "        y_all = y_all.float()\n",
        "    else:\n",
        "        x_all = x_all.float().numpy()\n",
        "        y_all = y_all.float().numpy()\n",
        "\n",
        "    return x_all, y_all\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyLNbZEIm56f"
      },
      "outputs": [],
      "source": [
        "# Input arguments\n",
        "args = pglm.arg_parser(jupyter=True)\n",
        "\n",
        "##### Modify default argments if needed #####\n",
        "args['base_dir']        = './Testing'\n",
        "args['fig_dir']         = './FigTesting'\n",
        "args['data_dir']        = './'\n",
        "args['date_ani']        = '011923/TestAni'\n",
        "args['stim_cond']       = 'Control'\n",
        "args['Nepochs']         = 50\n",
        "args['NoL1']            = True\n",
        "args['NoL2']            = False\n",
        "args['model_dt']        = 0\n",
        "\n",
        "params, exp = load_BaseModel_params(args=args,exp_dir_name='CustomData',ModelID=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqU3TYvkm56f"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "seed = 2\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "##### Generating data #####\n",
        "x_all,y_all = initialize_GP_inputs(Npats=1000,length_scale=5,batch_size=1,Nx_low=2,Nx=100,Ny_star=2,Nr=10,pytorch=True)\n",
        "x_all, y_all = x_all.squeeze(),y_all.squeeze()\n",
        "y_all = (y_all+1)/2\n",
        "x_all = (x_all - np.nanmean(x_all,axis=0))/np.nanstd(x_all,axis=0)\n",
        "\n",
        "##### Train/Test Splits ####\n",
        "gss = GroupShuffleSplit(n_splits=1, train_size=.8, random_state=42)\n",
        "frac = 0.1\n",
        "nT = x_all.shape[0]\n",
        "groups = np.hstack([i*np.ones(int((frac*i)*nT) - int((frac*(i-1))*nT)) for i in range(1,int(1/frac)+1)])\n",
        "train_idx, test_idx = next(iter(gss.split(np.arange(x_all.shape[0]), groups=groups)))\n",
        "# train_idx, test_idx = torch.from_numpy(train_idx), torch.from_numpy(test_idx)\n",
        "xtr,xte = x_all[train_idx], x_all[test_idx]\n",
        "xtr_pos,xte_pos = torch.zeros_like(xtr).float(),torch.zeros_like(xte).float()\n",
        "ytr,yte = y_all[train_idx], y_all[test_idx]\n",
        "\n",
        "print('X:',xtr.shape,'Xpos:',xtr_pos.shape,'y:',ytr.shape)\n",
        "print('X:',xte.shape,'Xpos:',xte_pos.shape,'y:',yte.shape)\n",
        "params['nk'] = xtr.shape[-1]\n",
        "params['Ncells'] = ytr.shape[-1]\n",
        "meanbias = torch.mean(y_all,dim=0)\n",
        "\n",
        "##### Create Datasets #####\n",
        "xtr, xte, xtr_pos, xte_pos, ytr, yte, meanbias=xtr.to(device), xte.to(device), xtr_pos.to(device), xte_pos.to(device), ytr.to(device), yte.to(device), meanbias.to(device)\n",
        "datasets = {\n",
        "            'xtr':xtr,\n",
        "            'xte':xte,\n",
        "            'xtr_pos':xtr_pos,\n",
        "            'xte_pos':xte_pos,\n",
        "            'ytr':ytr,\n",
        "            'yte':yte,\n",
        "            'meanbias':meanbias,\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QG9RWt47m56f"
      },
      "outputs": [],
      "source": [
        "###### Change some parameters for custom training #####\n",
        "params['initW'] = 'normal' #'zero' # 'normal'\n",
        "params['optimizer'] = 'sgd'\n",
        "network_config, initial_params = pglm.make_network_config(params,single_trial=0,custom=True)\n",
        "network_config['lr_w'] = .001\n",
        "network_config['lr_b'] = .1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tumAJs6Vm56g"
      },
      "outputs": [],
      "source": [
        "##### Train GLM on custon data #####\n",
        "tloss_trace,vloss_trace,model,optimizer = train_network(network_config,**datasets, params=params,filename=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1q0wMy5m56g"
      },
      "outputs": [],
      "source": [
        "##### Make prediction #####\n",
        "yhat = model(xte.to(device),xte_pos.to(device)).detach().cpu().numpy().squeeze()\n",
        "yt = yte.cpu().detach().numpy().squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYVOIuaJm56g"
      },
      "outputs": [],
      "source": [
        "###### Plotting loss and predictions #####\n",
        "fig, axs = plt.subplots(1,2,figsize=(10,4))\n",
        "ax = axs[0]\n",
        "cmap = pglm.discrete_cmap(vloss_trace.shape[-1],'jet')\n",
        "for cell in range(vloss_trace.shape[-1]):\n",
        "    ax.plot(vloss_trace[:,cell],c=cmap(cell))\n",
        "ax.set_xlabel('iteration')\n",
        "ax.set_ylabel('loss')\n",
        "sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=0, vmax=vloss_trace.shape[-1]))\n",
        "cbar = fig.colorbar(sm,ax=ax,format=None,shrink=0.7,pad=0.01)\n",
        "cbar.outline.set_linewidth(1)\n",
        "cbar.set_label('output dim')\n",
        "cbar.ax.tick_params(labelsize=12, width=1,direction='in')\n",
        "\n",
        "ncell = 0\n",
        "ax = axs[1]\n",
        "ax.plot(yt[:,ncell],c='k',label='actual')\n",
        "ax.plot(yhat[:,ncell],c='r',label='predicted')\n",
        "ax.set_xlabel('time')\n",
        "ax.set_ylabel('activity')\n",
        "ax.set_title('cc={:.03}'.format(np.corrcoef(yhat[:,ncell],yt[:,ncell])[1,0]))\n",
        "ax.legend(fontsize=10,ncol=1,labelcolor='linecolor', markerscale=0, handlelength=0, handletextpad=0,loc=\"upper left\",frameon=False, bbox_to_anchor=(.8, 1))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6VASoSNm56h"
      },
      "outputs": [],
      "source": [
        "###### Comparision with scikit-learn ######\n",
        "from sklearn.linear_model import LinearRegression\n",
        "x_all2,y_all2 = initialize_GP_inputs(Npats=1000,length_scale=5,batch_size=1,Nx_low=2,Nx=100,Ny_star=2,Nr=50,pytorch=False)\n",
        "x_all2, y_all2 = x_all2.squeeze(),y_all2.squeeze()\n",
        "y_all2 = (y_all2+1)/2\n",
        "x_all2 = x_all2/np.max(np.abs(x_all2))\n",
        "gss = GroupShuffleSplit(n_splits=1, train_size=.8, random_state=42)\n",
        "frac = 0.1\n",
        "nT = x_all2.shape[0]\n",
        "groups = np.hstack([i*np.ones(int((frac*i)*nT) - int((frac*(i-1))*nT)) for i in range(1,int(1/frac)+1)])\n",
        "train_idx, test_idx = next(iter(gss.split(np.arange(x_all2.shape[0]), groups=groups)))\n",
        "# train_idx, test_idx = torch.from_numpy(train_idx), torch.from_numpy(test_idx)\n",
        "xtr2,xte2 = x_all2[train_idx], x_all2[test_idx]\n",
        "xtr_pos2,xte_pos2 = np.zeros_like(xtr2),np.zeros_like(xte2)\n",
        "ytr2,yte2 = y_all2[train_idx], y_all2[test_idx]\n",
        "\n",
        "\n",
        "l1 = LinearRegression()\n",
        "l1.fit(xtr2,ytr2)\n",
        "yhat2 = l1.predict(xte2)\n",
        "print('cc=',np.corrcoef(yhat2[:,ncell],yte2[:,ncell])[0,1])\n",
        "\n",
        "fig, axs = plt.subplots(1,1,figsize=(4,4))\n",
        "ax = axs\n",
        "ax.plot(yt[:,ncell],c='k',label='actual')\n",
        "ax.plot(yhat2[:,ncell],c='r',label='predicted')\n",
        "ax.legend(fontsize=10,ncol=1,labelcolor='linecolor', markerscale=0, handlelength=0, handletextpad=0,loc=\"upper left\",frameon=False, bbox_to_anchor=(.8, 1))\n",
        "ax.set_xlabel('time')\n",
        "ax.set_ylabel('activity')\n",
        "ax.set_title('cc={:.03}'.format(np.corrcoef(yhat2[:,ncell],yt[:,ncell])[1,0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xjroyh7km56i"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o80yr5AFm56i"
      },
      "source": [
        "# Ray Tune Training: Parallel Cross Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPqIisham56i"
      },
      "source": [
        "For details about Ray Tune see: https://docs.ray.io/en/latest/tune/key-concepts.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4eC7Xlqm56i"
      },
      "outputs": [],
      "source": [
        "import ray\n",
        "from ray import tune\n",
        "from ray.air import session\n",
        "from ray.tune.search import ConcurrencyLimiter\n",
        "from ray.tune.search.hyperopt import HyperOptSearch\n",
        "from hyperopt import hp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjpnZ0dn7h8a"
      },
      "source": [
        "## Ray Tune for Custom Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBD0bfjGm56j"
      },
      "outputs": [],
      "source": [
        "# Input arguments\n",
        "args = pglm.arg_parser(jupyter=True)\n",
        "\n",
        "##### Modify default argments if needed #####\n",
        "args['base_dir']        = './Testing'\n",
        "args['fig_dir']         = './FigTesting'\n",
        "args['data_dir']        = './'\n",
        "args['date_ani']        = '011923/TestAni'\n",
        "args['stim_cond']       = 'Control'\n",
        "args['Nepochs']         = 50\n",
        "args['NoL1']            = True\n",
        "args['NoL2']            = True\n",
        "args['model_dt']        = 0\n",
        "\n",
        "params, exp = load_BaseModel_params(args=args,exp_dir_name='CustomData',ModelID=0)\n",
        "\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "##### Generating data #####\n",
        "x_all,y_all = initialize_GP_inputs(Npats=1000,length_scale=5,batch_size=1,Nx_low=2,Nx=100,Ny_star=2,Nr=10,pytorch=True)\n",
        "x_all, y_all = x_all.squeeze(),y_all.squeeze()\n",
        "y_all = (y_all+1)/2\n",
        "x_all = (x_all - np.nanmean(x_all,axis=0))/np.nanstd(x_all,axis=0)\n",
        "\n",
        "##### Train/Test Splits ####\n",
        "gss = GroupShuffleSplit(n_splits=1, train_size=.8, random_state=42)\n",
        "frac = 0.1\n",
        "nT = x_all.shape[0]\n",
        "groups = np.hstack([i*np.ones(int((frac*i)*nT) - int((frac*(i-1))*nT)) for i in range(1,int(1/frac)+1)])\n",
        "train_idx, test_idx = next(iter(gss.split(np.arange(x_all.shape[0]), groups=groups)))\n",
        "# train_idx, test_idx = torch.from_numpy(train_idx), torch.from_numpy(test_idx)\n",
        "xtr,xte = x_all[train_idx], x_all[test_idx]\n",
        "xtr_pos,xte_pos = torch.zeros_like(xtr).float(),torch.zeros_like(xte).float()\n",
        "ytr,yte = y_all[train_idx], y_all[test_idx]\n",
        "\n",
        "print('X:',xtr.shape,'Xpos:',xtr_pos.shape,'y:',ytr.shape)\n",
        "print('X:',xte.shape,'Xpos:',xte_pos.shape,'y:',yte.shape)\n",
        "\n",
        "params['nk'] = xtr.shape[-1]\n",
        "params['Ncells'] = ytr.shape[-1]\n",
        "meanbias = torch.mean(y_all,dim=0)\n",
        "xtr, xte, xtr_pos, xte_pos, ytr, yte, meanbias=xtr.to(device), xte.to(device), xtr_pos.to(device), xte_pos.to(device), ytr.to(device), yte.to(device), meanbias.to(device)\n",
        "datasets = {\n",
        "            'xtr':xtr,\n",
        "            'xte':xte,\n",
        "            'xtr_pos':xtr_pos,\n",
        "            'xte_pos':xte_pos,\n",
        "            'ytr':ytr,\n",
        "            'yte':yte,\n",
        "            'meanbias':meanbias,\n",
        "        }\n",
        "\n",
        "params['initW'] = 'normal' #'zero' # 'normal'\n",
        "params['optimizer'] = 'sgd'\n",
        "network_config, initial_params = pglm.make_network_config(params,custom=True)\n",
        "network_config['lr_w'] = tune.loguniform(1e-4, 1e-2)\n",
        "network_config['lr_b'] = tune.loguniform(1e-2, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlxwmCVem56j"
      },
      "source": [
        "`network_config` contains the key parameters for the network and this is where we can choose which hyperparameters to optimize. In this example, we will optimize the learning rate of the weights and bias of the network to get the best fit. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4RUTCQ5m56j"
      },
      "outputs": [],
      "source": [
        "##### Defining initial params for hyperparam search #####\n",
        "initial_params = [\n",
        "    {\"lr_w\": 0.001,\"lr_b\": 0.1, },\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHMQsk6wm56j"
      },
      "outputs": [],
      "source": [
        "ray.init(ignore_reinit_error=True,include_dashboard=True)\n",
        "\n",
        "##### Define Search Algorithm for hyperparam search #####\n",
        "algo = HyperOptSearch(points_to_evaluate=initial_params)\n",
        "algo = ConcurrencyLimiter(algo, max_concurrent=4)\n",
        "num_samples = 10\n",
        "\n",
        "##### Set up tuner #####\n",
        "sync_config = tune.SyncConfig()  # the default mode is to use use rsync\n",
        "tuner = tune.Tuner(\n",
        "    tune.with_resources(\n",
        "        tune.with_parameters(train_network,**datasets, params=params),\n",
        "        resources={\"cpu\": 2, \"gpu\": .5}),\n",
        "    tune_config=tune.TuneConfig(metric=\"avg_loss\",mode=\"min\",search_alg=algo,num_samples=num_samples),\n",
        "    param_space=network_config,\n",
        "    run_config=air.RunConfig(local_dir=params['save_model'], name=\"NetworkAnalysis\",sync_config=sync_config,verbose=2)\n",
        ")\n",
        "\n",
        "##### perform fits #####\n",
        "results = tuner.fit()\n",
        "\n",
        "best_result = results.get_best_result(\"avg_loss\", \"min\")\n",
        "print(\"Best trial config: {}\".format(best_result.config))\n",
        "print(\"Best trial final validation loss: {}\".format(best_result.metrics[\"avg_loss\"]))\n",
        "##### Get experiment dataframe and best network\n",
        "df = results.get_dataframe()\n",
        "best_network = list(params['save_model'].rglob('*{}.pt'.format(best_result.metrics['trial_id'])))[0]\n",
        "\n",
        "##### Save experiment data and save best network as h5 #####\n",
        "exp_filename = '_'.join([params['model_type'],params['data_name']]) + 'experiment_data.h5'\n",
        "exp_best_dict = {'best_network':best_network,'trial_id':best_result.metrics['trial_id'],'best_config':best_result.config}\n",
        "pglm.h5store(params['save_model'] / ('NetworkAnalysis/{}'.format(exp_filename)), df, **exp_best_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8j0ryfm0m56k"
      },
      "outputs": [],
      "source": [
        "#@title Custom Evaluate_network Fuction\n",
        "\n",
        "def custom_eval(best_network,network_config,params,xte,xte_pos,yte,device='cpu'):\n",
        "    \"\"\"Evaluates ray tune experiment and hyperparameter search\n",
        "\n",
        "    Args:\n",
        "        best_network (str): path to best network model '.pt' file\n",
        "        network_config (dict): network_config for best network\n",
        "        params (dict): key parameters dictionary\n",
        "        xte (Tensor): test input data \n",
        "        xte_pos (Tensor): test additional input data\n",
        "        yte (Tensor): target test data\n",
        "        device (str, optional): device to load data onto. Defaults to 'cpu'.\n",
        "    \"\"\"\n",
        "    import pytorchGLM.Utils.io_dict_to_hdf5 as ioh5\n",
        "    from pytorchGLM.main.models import model_wrapper,BaseModel\n",
        "\n",
        "    ##### Load best network from saved ray experiment ######\n",
        "    state_dict, _ = torch.load(best_network,map_location='cpu')\n",
        "    model = model_wrapper((network_config,BaseModel))\n",
        "    model.load_state_dict(state_dict)\n",
        "    model.to(device)\n",
        "\n",
        "    ##### Load data into device and predict test set ######\n",
        "    xte, xte_pos, yte = xte.to(device), xte_pos.to(device), yte.to(device)\n",
        "    with torch.no_grad():\n",
        "        yhat = model(xte,xte_pos)\n",
        "\n",
        "    ##### Smooth Firing rates and save ######\n",
        "    actual_smooth = yte.detach().cpu().numpy()\n",
        "    pred_smooth = yhat.detach().cpu().numpy()\n",
        "    cc_test = np.array([(np.corrcoef(pred_smooth[:,celln],actual_smooth[:,celln])[0, 1]) for celln in range(pred_smooth.shape[1])])\n",
        "    \n",
        "    GLM_Dict = {\n",
        "        'actual_smooth': actual_smooth,\n",
        "        'pred_smooth': pred_smooth,\n",
        "        'cc_test': cc_test,\n",
        "        }\n",
        "\n",
        "    for key in state_dict.keys():\n",
        "        GLM_Dict[key]  = state_dict[key].cpu().numpy()\n",
        "\n",
        "    model_name = '{}_ModelID{:d}_dt{:03d}_T{:02d}_NB{}_Best.h5'.format(params['model_type'], params['ModelID'],int(params['model_dt']*1000), params['nt_glm_lag'], params['Nepochs'])\n",
        "    ioh5.save(params['save_model']/model_name,GLM_Dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Custom Evaluate_network Fuction\n",
        "\n",
        "def custom_eval(df,network_config,params,xte,xte_pos,yte,device='cpu'):\n",
        "    \"\"\"Evaluates ray tune experiment and hyperparameter search\n",
        "\n",
        "    Args:\n",
        "        best_network (str): path to best network model '.pt' file\n",
        "        network_config (dict): network_config for best network\n",
        "        params (dict): key parameters dictionary\n",
        "        xte (Tensor): test input data \n",
        "        xte_pos (Tensor): test additional input data\n",
        "        yte (Tensor): target test data\n",
        "        device (str, optional): device to load data onto. Defaults to 'cpu'.\n",
        "    \"\"\"\n",
        "    import pytorchGLM.Utils.io_dict_to_hdf5 as ioh5\n",
        "    from pytorchGLM.main.models import model_wrapper,BaseModel\n",
        "\n",
        "    ##### Load best network from saved ray experiment ######\n",
        "    state_dict, _ = torch.load(best_network,map_location='cpu')\n",
        "    model = model_wrapper((network_config,BaseModel))\n",
        "    model.load_state_dict(state_dict)\n",
        "    model.to(device)\n",
        "\n",
        "    ##### Load data into device and predict test set ######\n",
        "    xte, xte_pos, yte = xte.to(device), xte_pos.to(device), yte.to(device)\n",
        "    with torch.no_grad():\n",
        "        yhat = model(xte,xte_pos)\n",
        "\n",
        "    ##### Smooth Firing rates and save ######\n",
        "    actual_smooth = yte.detach().cpu().numpy()\n",
        "    pred_smooth = yhat.detach().cpu().numpy()\n",
        "    cc_test = np.array([(np.corrcoef(pred_smooth[:,celln],actual_smooth[:,celln])[0, 1]) for celln in range(pred_smooth.shape[1])])\n",
        "    \n",
        "    GLM_Dict = {\n",
        "        'actual_smooth': actual_smooth,\n",
        "        'pred_smooth': pred_smooth,\n",
        "        'cc_test': cc_test,\n",
        "        }\n",
        "\n",
        "    for key in state_dict.keys():\n",
        "        GLM_Dict[key]  = state_dict[key].cpu().numpy()\n",
        "\n",
        "    model_name = '{}_ModelID{:d}_dt{:03d}_T{:02d}_NB{}_Best.h5'.format(params['model_type'], params['ModelID'],int(params['model_dt']*1000), params['nt_glm_lag'], params['Nepochs'])\n",
        "    ioh5.save(params['save_model']/model_name,GLM_Dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7IFtc9Vm56l"
      },
      "outputs": [],
      "source": [
        "custom_eval(best_network,network_config,params,xte,xte_pos,yte,device='cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kj6G4ZhXm56l"
      },
      "outputs": [],
      "source": [
        "exp_filename = list((params['save_model']/'NetworkAnalysis').rglob('*experiment_data.h5'))[0]\n",
        "df,meta_data = pglm.h5load(exp_filename)\n",
        "best_network=meta_data['best_network']\n",
        "best_config = meta_data['best_config']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-b6TWBHHm56l"
      },
      "outputs": [],
      "source": [
        "import pytorchGLM.Utils.io_dict_to_hdf5 as ioh5\n",
        "model_name = '{}_ModelID{:d}_dt{:03d}_T{:02d}_NB{}_Best.h5'.format(params['model_type'], params['ModelID'],int(params['model_dt']*1000), params['nt_glm_lag'], params['Nepochs'])\n",
        "GLM_Data = ioh5.load(params['save_model']/model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GngIhvym56m"
      },
      "outputs": [],
      "source": [
        "GLM_Data.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvXwEcKim56m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMDZtlUpQg9t"
      },
      "source": [
        "## Training Ray Tune Hyper Params Search Neill Lab data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agBSvHeeQg9v"
      },
      "outputs": [],
      "source": [
        "# Input arguments\n",
        "args = pglm.arg_parser(jupyter=True)\n",
        "\n",
        "##### Modify default argments if needed #####\n",
        "dates_all = ['070921/J553RT' ,'101521/J559NC','102821/J570LT','110421/J569LT'] #,'122021/J581RT','020422/J577RT'] # '102621/J558NC' '062921/G6HCK1ALTRN',\n",
        "args['date_ani']        = dates_all[0]\n",
        "args['base_dir']        = './Testing'\n",
        "args['fig_dir']         = './FigTesting'\n",
        "args['data_dir']        = './'\n",
        "args['free_move']       = True\n",
        "args['train_shifter']   = False\n",
        "args['Nepochs']         = 1000\n",
        "args['num_samples']     = 2\n",
        "args['cpus_per_task']   = 2\n",
        "args['gpus_per_task']   = .5\n",
        "\n",
        "ModelID = 1\n",
        "params, file_dict, exp = pglm.load_params(args,ModelID,file_dict={},exp_dir_name=None,nKfold=0,debug=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irI2gOyhQg9w"
      },
      "outputs": [],
      "source": [
        "##### Load Niell Lab Dataset #####\n",
        "data = pglm.load_aligned_data(file_dict, params, reprocess=False)\n",
        "params = pglm.get_modeltype(params)\n",
        "datasets, network_config,initial_params = pglm.load_datasets(file_dict,params,single_trial=None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEWE6bcZQg9x"
      },
      "outputs": [],
      "source": [
        "##### Define Search Algorithm for hyperparam search #####\n",
        "algo = HyperOptSearch(points_to_evaluate=initial_params)\n",
        "algo = ConcurrencyLimiter(algo, max_concurrent=4)\n",
        "\n",
        "##### Set up tuner #####\n",
        "num_samples = args['num_samples']\n",
        "sync_config = tune.SyncConfig()  # the default mode is to use use rsync\n",
        "tuner = tune.Tuner(\n",
        "    tune.with_resources(\n",
        "        tune.with_parameters(pglm.train_network,**datasets,params=params),\n",
        "        resources={\"cpu\":args['cpus_per_task'], \"gpu\": args['gpus_per_task']}),\n",
        "    tune_config=tune.TuneConfig(metric=\"avg_loss\",mode=\"min\",search_alg=algo,num_samples=num_samples),\n",
        "    param_space=network_config,\n",
        "    run_config=air.RunConfig(local_dir=params['save_model'], name=\"NetworkAnalysis\",sync_config=sync_config)\n",
        ")\n",
        "\n",
        "##### Run hyperparam search #####\n",
        "results = tuner.fit()\n",
        "\n",
        "##### Collect best result #####\n",
        "best_result = results.get_best_result(\"avg_loss\", \"min\")\n",
        "print(\"Best trial config: {}\".format(best_result.config))\n",
        "print(\"Best trial final validation loss: {}\".format(best_result.metrics[\"avg_loss\"]))\n",
        "df = results.get_dataframe()\n",
        "\n",
        "##### Save best network ######\n",
        "best_network = list(params['save_model'].rglob('*{}.pt'.format(best_result.metrics['trial_id'])))[0]\n",
        "exp_filename = '_'.join([params['model_type'],params['data_name_fm']]) + 'experiment_data.h5'\n",
        "exp_best_dict = {'best_network':best_network,'trial_id':best_result.metrics['trial_id'],'best_config':best_result.config}\n",
        "pglm.h5store(params['save_model'] / ('NetworkAnalysis/{}'.format(exp_filename)), df, **exp_best_dict)\n",
        "\n",
        "##### Evaluate hyperparameter search #####\n",
        "pglm.evaluate_networks(best_network,best_result.config,params,datasets['xte'],datasets['xte_pos'],datasets['yte'],device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4JP2xGjQg9x"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZSTtX4eCA1T"
      },
      "source": [
        "# Searching for files to load "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jDPwylECEJH"
      },
      "outputs": [],
      "source": [
        "import pytorchGLM.Utils.io_dict_to_hdf5 as ioh5\n",
        "# Input arguments\n",
        "args = pglm.arg_parser(jupyter=True)\n",
        "\n",
        "##### Modify default argments if needed #####\n",
        "dates_all = ['070921/J553RT' ,'101521/J559NC','102821/J570LT','110421/J569LT'] #,'122021/J581RT','020422/J577RT'] # '102621/J558NC' '062921/G6HCK1ALTRN',\n",
        "args['date_ani']        = dates_all[0]\n",
        "args['base_dir']        = './Testing'\n",
        "args['fig_dir']         = './FigTesting'\n",
        "args['data_dir']        = './'\n",
        "args['free_move']       = True\n",
        "args['train_shifter']   = False\n",
        "args['Nepochs']         = 1000\n",
        "args['num_samples']     = 2\n",
        "args['cpus_per_task']   = 2\n",
        "args['gpus_per_task']   = .5\n",
        "\n",
        "ModelID = 1\n",
        "params, file_dict, exp = pglm.load_params(args,ModelID,file_dict={},exp_dir_name=None,nKfold=0,debug=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bWC9m73COIO"
      },
      "outputs": [],
      "source": [
        "model_list = list(params['save_model'].rglob('*Best.h5'))\n",
        "\n",
        "model_name = model_list[0]\n",
        "\n",
        "GLM_Data = ioh5.load(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPwdGv9lEYGF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gixgN3bJGiQ0"
      },
      "source": [
        "# Using Ray for parallelized animations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hhosaQJRH8Q-"
      },
      "outputs": [],
      "source": [
        "#@title Progress Bar\n",
        "import gc\n",
        "import cv2\n",
        "from asyncio import Event\n",
        "from typing import Tuple\n",
        "from ray.actor import ActorHandle\n",
        "from tqdm import tqdm\n",
        "\n",
        "@ray.remote\n",
        "class ProgressBarActor:\n",
        "    counter: int\n",
        "    delta: int\n",
        "    event: Event\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self.counter = 0\n",
        "        self.delta = 0\n",
        "        self.event = Event()\n",
        "\n",
        "    def update(self, num_items_completed: int) -> None:\n",
        "        \"\"\"Updates the ProgressBar with the incremental\n",
        "        number of items that were just completed.\n",
        "        \"\"\"\n",
        "        self.counter += num_items_completed\n",
        "        self.delta += num_items_completed\n",
        "        self.event.set()\n",
        "\n",
        "    async def wait_for_update(self) -> Tuple[int, int]:\n",
        "        \"\"\"Blocking call.\n",
        "\n",
        "        Waits until somebody calls `update`, then returns a tuple of\n",
        "        the number of updates since the last call to\n",
        "        `wait_for_update`, and the total number of completed items.\n",
        "        \"\"\"\n",
        "        await self.event.wait()\n",
        "        self.event.clear()\n",
        "        saved_delta = self.delta\n",
        "        self.delta = 0\n",
        "        return saved_delta, self.counter\n",
        "\n",
        "    def get_counter(self) -> int:\n",
        "        \"\"\"\n",
        "        Returns the total number of complete items.\n",
        "        \"\"\"\n",
        "        return self.counter\n",
        "    \n",
        "\n",
        "class ProgressBar:\n",
        "    progress_actor: ActorHandle\n",
        "    total: int\n",
        "    description: str\n",
        "    pbar: tqdm\n",
        "\n",
        "    def __init__(self, total: int, description: str = \"\"):\n",
        "        # Ray actors don't seem to play nice with mypy, generating\n",
        "        # a spurious warning for the following line,\n",
        "        # which we need to suppress. The code is fine.\n",
        "        self.progress_actor = ProgressBarActor.remote()  # type: ignore\n",
        "        self.total = total\n",
        "        self.description = description\n",
        "\n",
        "    @property\n",
        "    def actor(self) -> ActorHandle:\n",
        "        \"\"\"Returns a reference to the remote `ProgressBarActor`.\n",
        "\n",
        "        When you complete tasks, call `update` on the actor.\n",
        "        \"\"\"\n",
        "        return self.progress_actor\n",
        "\n",
        "    def print_until_done(self) -> None:\n",
        "        \"\"\"Blocking call.\n",
        "\n",
        "        Do this after starting a series of remote Ray tasks, to which you've\n",
        "        passed the actor handle. Each of them calls `update` on the actor.\n",
        "        When the progress meter reaches 100%, this method returns.\n",
        "        \"\"\"\n",
        "        pbar = tqdm(desc=self.description, total=self.total)\n",
        "        while True:\n",
        "            delta, counter = ray.get(self.actor.wait_for_update.remote())\n",
        "            pbar.update(delta)\n",
        "            if counter >= self.total:\n",
        "                pbar.close()\n",
        "                return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2Z8DUG2G3re"
      },
      "outputs": [],
      "source": [
        "import pytorchGLM.Utils.io_dict_to_hdf5 as ioh5\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "# Input arguments\n",
        "args = pglm.arg_parser(jupyter=True)\n",
        "\n",
        "##### Modify default argments if needed #####\n",
        "dates_all = ['070921/J553RT' ,'101521/J559NC','102821/J570LT','110421/J569LT'] #,'122021/J581RT','020422/J577RT'] # '102621/J558NC' '062921/G6HCK1ALTRN',\n",
        "args['date_ani']        = dates_all[0]\n",
        "args['base_dir']        = './Testing'\n",
        "args['fig_dir']         = './FigTesting'\n",
        "args['data_dir']        = './'\n",
        "args['free_move']       = True\n",
        "args['train_shifter']   = False\n",
        "args['Nepochs']         = 1000\n",
        "args['num_samples']     = 2\n",
        "args['cpus_per_task']   = 2\n",
        "args['gpus_per_task']   = .5\n",
        "\n",
        "ModelID = 1\n",
        "params, file_dict, exp = pglm.load_params(args,ModelID,file_dict={},exp_dir_name=None,nKfold=0,debug=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OicQ6aygHEHu"
      },
      "outputs": [],
      "source": [
        "data = pglm.load_aligned_data(file_dict, params, reprocess=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KE5zLBM5HHoR"
      },
      "outputs": [],
      "source": [
        "model_vid_sm = data['model_vid_sm_shift']\n",
        "model_gz, model_th, model_phi = data['model_gz'], data['model_th'], data['model_phi']\n",
        "model_dt = params['model_dt']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lIKC1XaGk2-"
      },
      "outputs": [],
      "source": [
        "t = 10\n",
        "time_range = np.arange(0,int(60//model_dt))\n",
        "\n",
        "fig2 = plt.figure(constrained_layout=False, figsize=(10, 10),dpi=90)\n",
        "spec2 = gridspec.GridSpec(ncols=2, nrows=4, figure=fig2)\n",
        "axs1 = fig2.add_subplot(spec2[:2, :])\n",
        "inner_grid = gridspec.GridSpecFromSubplotSpec(3, 1, spec2[2:, :],hspace=.5)\n",
        "axs2a = fig2.add_subplot(inner_grid[0, 0])\n",
        "axs2b = fig2.add_subplot(inner_grid[1, 0])\n",
        "axs2c = fig2.add_subplot(inner_grid[2, 0])\n",
        "width, height = fig2.get_size_inches() * fig2.get_dpi()\n",
        "# Image frame \n",
        "axs1.imshow(model_vid_sm[t], cmap='gray')\n",
        "axs1.axis('off')\n",
        "\n",
        "# Movement Params\n",
        "axs2a.plot(time_range*model_dt,model_gz[time_range],'k',lw=3)\n",
        "axs2a.axvline(x=t*model_dt,c='b')\n",
        "axs2a.set_title('Gyro z')\n",
        "# axs2a.set_xlabel('Time (sec)')\n",
        "axs2b.plot(time_range*model_dt,model_th[time_range], 'k', lw=3)\n",
        "axs2b.axvline(x=t*model_dt,c='b')\n",
        "axs2b.set_title('Eye Theta')\n",
        "# axs2b.set_xlabel('Time (sec)')\n",
        "axs2c.plot(time_range*model_dt,model_phi[time_range], 'k', lw=3)\n",
        "axs2c.axvline(x=t*model_dt,c='b')\n",
        "axs2c.set_title('Eye Phi')\n",
        "axs2c.set_xlabel('Time (sec)')\n",
        "plt.tight_layout()\n",
        "fig2.canvas.draw()       # draw the canvas, cache the renderer\n",
        "images = np.frombuffer(fig2.canvas.tostring_rgb(), dtype='uint8').reshape(int(height), int(width), 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQjkL27nGzA_"
      },
      "outputs": [],
      "source": [
        "\n",
        "@ray.remote\n",
        "def make_plt_im(t, time_range, model_vid_sm, model_gz, model_th, model_phi, pbar):  #\n",
        "    fig2 = plt.figure(constrained_layout=False, figsize=(10, 10), dpi=90)\n",
        "    spec2 = gridspec.GridSpec(ncols=2, nrows=4, figure=fig2)\n",
        "    axs1 = fig2.add_subplot(spec2[:2, :])\n",
        "    inner_grid = gridspec.GridSpecFromSubplotSpec(3, 1, spec2[2:, :], hspace=.5)\n",
        "    axs2a = fig2.add_subplot(inner_grid[0, 0])\n",
        "    axs2b = fig2.add_subplot(inner_grid[1, 0])\n",
        "    axs2c = fig2.add_subplot(inner_grid[2, 0])\n",
        "    # Image frame\n",
        "    axs1.imshow(model_vid_sm[t], cmap='gray')\n",
        "    axs1.axis('off')\n",
        "\n",
        "    # Movement Params\n",
        "    axs2a.plot(time_range*model_dt, model_gz[time_range], 'k', lw=3)\n",
        "    axs2a.axvline(x=t*model_dt, c='b')\n",
        "    axs2a.set_title('Gyro z')\n",
        "    axs2b.plot(time_range*model_dt, model_th[time_range], 'k', lw=3)\n",
        "    axs2b.axvline(x=t*model_dt, c='b')\n",
        "    axs2b.set_title('Eye Theta')\n",
        "    axs2c.plot(time_range*model_dt, model_phi[time_range], 'k', lw=3)\n",
        "    axs2c.axvline(x=t*model_dt, c='b')\n",
        "    axs2c.set_title('Eye Phi')\n",
        "    axs2c.set_xlabel('Time (sec)')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    width, height = fig2.get_size_inches() * fig2.get_dpi()\n",
        "    fig2.canvas.draw()       # draw the canvas, cache the renderer\n",
        "    images = np.frombuffer(fig2.canvas.tostring_rgb(),\n",
        "                        dtype='uint8').reshape(int(height), int(width), 3)\n",
        "    \n",
        "    plt.close()\n",
        "    pbar.update.remote(1)\n",
        "    return images\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9XKMES1GzDW"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "mpl.use('agg')\n",
        "\n",
        "##### initialize time points for animation and progressbar #####\n",
        "t = 0\n",
        "time_range = np.arange(t,t+int(60//model_dt))\n",
        "num_ticks = np.size(time_range)\n",
        "pb = ProgressBar(num_ticks)\n",
        "actor = pb.actor\n",
        "\n",
        "##### Put large arrays into shared memory #####\n",
        "time_range_r = ray.put(time_range)\n",
        "model_vid_sm_r = ray.put(model_vid_sm)\n",
        "model_gz_r = ray.put(model_gz)\n",
        "model_th_r = ray.put(model_th)\n",
        "model_phi_r = ray.put(model_phi)\n",
        "\n",
        "\n",
        "##### Loop over parameters appending process ids #####\n",
        "result_ids = []\n",
        "for t in time_range:\n",
        "    result_ids.append(make_plt_im.remote(t, time_range_r, model_vid_sm_r, model_gz_r, model_th_r, model_phi_r, actor))\n",
        "\n",
        "##### pring progressbar and get results #####\n",
        "pb.print_until_done()\n",
        "results_p = ray.get(result_ids)\n",
        "images = np.stack([results_p[i] for i in range(len(results_p))])\n",
        "\n",
        "##### Make video with opencv #####\n",
        "aniname = params['date_ani2'] + '_summary.mp4'\n",
        "vid_name = params['fig_dir'] / aniname\n",
        "FPS = int(1/model_dt)\n",
        "out = cv2.VideoWriter(vid_name.as_posix(), cv2.VideoWriter_fourcc(*'mp4v'), FPS, (images.shape[-2], images.shape[-3]))\n",
        "\n",
        "for fm in range(images.shape[0]):\n",
        "    out.write(cv2.cvtColor(images[fm], cv2.COLOR_BGR2RGB))\n",
        "out.release()\n",
        "\n",
        "\n",
        "del results_p, time_range_r, model_vid_sm_r, model_gz_r, model_th_r, model_phi_r, pb\n",
        "gc.collect()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14oiWQ6RIgQr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "GMDZtlUpQg9t",
        "kZSTtX4eCA1T",
        "gixgN3bJGiQ0"
      ],
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "pytorchGLM_test",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "aeb3b4f9986e9903770e85426a5ea2a9c74e9cc9c9c2883aec8e061002a2af84"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
